{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required classes \n",
    "from pypdf import PdfReader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emotion Embeddings - text Multi-way Mapping model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pdf reader object \n",
    "reader = PdfReader('C:\\\\Users\\\\josep\\\\Downloads\\\\Emotion embeddings - Learning Stable and Homogeneous Abstractions from Heterogeneous Affective Datasets.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "NON-ARCHIVAL PREPRINT 7\n",
      "Algorithm 1 Training the Multi-Way Mapping Model\n",
      "1:(Y1,1, Y1,2),(Y2,1, Y2,2), . . .(Yn,1, Yn,2)←Mapping datasets used for training\n",
      "2:(C1,1,C1,2),(C2,1,C2,2). . . ,(Cn,1,Cn,2)←loss criteria for mapping datasets\n",
      "3:(α1,1, α1,2),(α2,1, α2,2), . . .(αn,1, αn,2)←weight terms to balance between loss criteria\n",
      "4:(g1,1, h1,1, g1,2, h1,2),(g2,1, h2,1, g2,2, h2,2), . . . , (gn,1, hn,1, gn,2, hn,2)←randomly initialized label encoders and predic-\n",
      "tion heads†\n",
      "5:K={K1, K2, . . . , K m} ← Set of equivalence classes of emotion variables, where each equivalence class Kjis a set of\n",
      "row vectors from weight matrices of different prediction heads that refer to equivalent emotion variables (see Table 3).\n",
      "6:nsteps←total number of training steps\n",
      "7:for all kin1, . . . , n steps do\n",
      "8:i←randomly sample index of one of the mapping datasets ∈ {1, . . . , n }\n",
      "9:(y1, y2)←randomly sample a batch s.t. y1⊂Yi,1andy2⊂Yi,2with identical indices\n",
      "10: (e1, e2)←(gi,1(y1), gi,2(y2))\n",
      "11: ˆy1,1←hi,1(e1)\n",
      "12: ˆy1,2←hi,2(e1)\n",
      "13: ˆy2,1←hi,1(e2)\n",
      "14: ˆy2,2←hi,2(e2)\n",
      "15: Lmap← Ci,1(y1,ˆy2,1) +Ci,2(y2,ˆy1,2)\n",
      "16: Lauto← C(y1,ˆy1,1) +C(y2,ˆy2,2)\n",
      "17: Lsim← C sim(e1, e2)\n",
      "18: Lpara←P\n",
      "K∈KP\n",
      "{u,v|u,v∈K,u̸=v}1−cos(u, v)\n",
      "19: Ltotal←Lmap+Lauto+Lsim+Lpara\n",
      "20: compute ∇Ltotal and update weights\n",
      "21:end for\n",
      "†If two sets of labels Ya,b, Yc,dfollow the same label format, then they use the same label encoders and prediction\n",
      "heads, i.e, ga,b=gc,dandha,b=hc,d. The same is true for loss criteria Cand weight terms α.\n",
      "Y2:={y2,1, y2,2, . . . y 2,n}, respectively. Typically, they are\n",
      "constructed by matching instances from independent anno-\n",
      "tation studies (e.g., the first two rows in Table 1). Thus, we\n",
      "can think of the two sets of labels as “translational equiv-\n",
      "alents”, i.e., differently formatted emotion ratings, possibly\n",
      "capturing different affective nuances, yet still describing the\n",
      "same underlying emotional state.\n",
      "The intuition behind our training scheme is to “fuse”\n",
      "multiple mapping models by forcing them to produce the\n",
      "same intermediate representation for both mapping direc-\n",
      "tions. This results in a multi-way mapping model with\n",
      "a shared representation layer in the middle, the common\n",
      "emotion space (Fig. 2 b).\n",
      "Formally, let (Y1, Y2)be a mapping dataset with a sample\n",
      "(y1, y2)following label formats L1,L2(subscripts refer to\n",
      "label format). Our goal is to align the intermediate represen-\n",
      "tations, e1, e2:=g1(y1),g2(y2)while also deriving accurate\n",
      "mapping predictions. Therefore, we propose the following\n",
      "ℎ!ℎ\"𝑔\"𝑔!#𝑦\",\"\n",
      "𝑦!𝑦\"#𝑦!,\"𝑒\"𝑒!#𝑦!,!#𝑦\",!𝐿sim𝐿auto𝐿auto𝐿map𝐿map𝐿para\n",
      "Fig. 3. Training the Multi-Way Mapping Model.three training objectives (see Fig. 3):\n",
      "The first training objective is the mapping loss Lmapwhere\n",
      "we compare true vs. predicted labels.\n",
      "Lmap:=α1C1[y1, h1(g2(y2))] + α2C2[y2, h2(g1(y1))] (1)\n",
      "The two summands represent the two mapping directions,\n",
      "assigning either of the two labels as the source, the other\n",
      "as the target format. Cidenotes the loss criterion for label\n",
      "format Liandαiis the format-specific weight term which\n",
      "is meant to re-balance loss criteria that otherwise tend to\n",
      "produce outputs of vastly different magnitude. As loss\n",
      "criteria, we use cross-entropy loss for multi-class single-label\n",
      "problems, binary cross-entropy loss for multi-class multi-\n",
      "label problems, and Mean-Squared-Error loss for regression\n",
      "problems.\n",
      "Next, the autoencoder loss ,Lauto, captures how well the\n",
      "model can reconstruct the original input label from the\n",
      "hidden emotion representation. It is meant to supplement\n",
      "the mapping loss:\n",
      "Lauto:=α1C1[y1, h1(g1(y1))] + α2C2[y2,(h2(g2(y2))] (2)\n",
      "Lastly, the similarity loss ,Lsim, directly assesses whether\n",
      "both input label formats end up with a similar intermediate\n",
      "representation:\n",
      "Lsim:=Csim[g1(y1), g2(y2)] (3)\n",
      "where Csimdenotes Mean-Squared-Error loss.\n",
      "Besides these local loss terms which only take into ac-\n",
      "count encoders and decoders used in processing a particular\n",
      "instance (pair of labels), we also introduce a parameter\n",
      "sharing loss Lpara that globally takes into account alllabel\n"
     ]
    }
   ],
   "source": [
    "# printing number of pages in pdf file \n",
    "print(len(reader.pages)) \n",
    "\n",
    "# creating a page object \n",
    "page = reader.pages[6] \n",
    "\n",
    "# extracting text from page \n",
    "print(page.extract_text()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fong's Music theory model for pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader('C:\\\\Users\\\\josep\\\\Downloads\\\\A Theory-Based Interpretable Deep Learning Architecture for Music Emotion.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "Interpretable Deep Learning for Music Emotion 9\n",
      "model. The key implication that arises is that ﬁlter design must be done thoughtfully in\n",
      "advance in order to ensure interpretability, and not just ex post after training the model.\n",
      "3. Model\n",
      "We develop a deep learning model based on convolutional neural networks for emotion clas-\n",
      "siﬁcation that includes several theoretically motivated components relating to the physics\n",
      "of sound waves and the perception of music by listeners. For a detailed deﬁnition of the\n",
      "music terms used, please see Table A1 in Section A of the Appendix.\n",
      "We characterize emotion using the valence-arousal circumplex model represented in Fig-\n",
      "ure 1, based on Russell (1980). Valence measures how positive or negative a listener feels\n",
      "and higher valence maps to a more positive feeling. Arousal measures how energetic a\n",
      "listener feels and higher arousal maps to greater excitement and energy. Discrete emotions\n",
      "such as “happy,” “sad,” “relaxed,” and “alarmed” can be mapped onto the valence and\n",
      "arousal dimensions. Both the discrete and dimensional models have been used in the liter-\n",
      "ature. The main drawbacks of the discrete model are that researchers have yet to reach a\n",
      "consensus on the appropriate level of emotional granularity for music and there is ambigu-\n",
      "ity in language (Yang and Chen 2011b). In contrast, the valence-arousal model implicitly\n",
      "oﬀers an inﬁnite number of emotion descriptions and as a result, many researchers have\n",
      "adopted the valence-arousal framework for emotion classiﬁcation (e.g., Panda et al. 2018,\n",
      "Yang and Chen 2011a, MacDorman 2007, Korhonen et al. 2006).\n",
      "Figure 1 Russell’s Circumplex Model of Emotion\n",
      "We begin with an overview of the steps of our deep learning model that maps music to\n",
      "emotion in Figure 2. Step S1takes six seconds of raw audio sound wave data as input. In\n",
      "StepS2, the music clip is converted to a short-time Fourier transform (STFT) spectrogram,\n",
      "which is a visual representation of the frequencies present in the sound wave. In Step S3,\n",
      "we transform the STFT spectrogram to a mel spectrogram, which characterizes how the\n"
     ]
    }
   ],
   "source": [
    "# printing number of pages in pdf file \n",
    "print(len(reader.pages)) \n",
    "\n",
    "# creating a page object \n",
    "page = reader.pages[9] \n",
    "\n",
    "# extracting text from page \n",
    "print(page.extract_text()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
